{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic_spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKelUpuqWJ12v2EiHzoRzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldselvera/titanic_classification_spark/blob/main/titanic_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark "
      ],
      "metadata": {
        "id": "lSwtcPsoPdg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is a system that provides a cluster-based distributed computing environment with the help of its packages, including:\n",
        "*   SQL querying\n",
        "*   streaming data processing\n",
        "*   machine learning"
      ],
      "metadata": {
        "id": "oeV4tzG_Qrdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Installation"
      ],
      "metadata": {
        "id": "7c9YdS_BQ7OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-ZkTwaRQLvo",
        "outputId": "a693e5dc-62cc-437e-c277-e49cbeff8ec8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=eb8aaf73956cd515281bdbd2575d803aa38c0a73265759a2f31f428788344898\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Session"
      ],
      "metadata": {
        "id": "I4P9oNIPRB6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import StringIndexer"
      ],
      "metadata": {
        "id": "I4OJ4WaHQQXX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"FirstSparkApplication\").config (\"spark.executor.memory\", \"8g\").getOrCreate()"
      ],
      "metadata": {
        "id": "a35H1z7URElW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "QpyBMPu6RQbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “Ttanic” dataset will be used and may be downloaded from Kaggle website [here](https://www.kaggle.com/c/titanic/data)."
      ],
      "metadata": {
        "id": "R7fTDW9iRlpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('/content/titanic_train.csv')\n",
        "test_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('/content/test.csv')\n",
        "\n",
        "training_dataset.printSchema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsnVcgiaQKLo",
        "outputId": "0de28809-5f48-4845-94ed-c95ed780345f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Passenger Counts\")\n",
        "training_dataset.agg(countDistinct(\"PassengerId\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxcr-TurdIQu",
        "outputId": "ff39be72-03e4-48e6-9428-b419f4f52a0d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Passenger Counts\n",
            "+------------------+\n",
            "|count(PassengerId)|\n",
            "+------------------+\n",
            "|               891|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Dataset Row Count\")\n",
        "test_dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_faAjss0dcVJ",
        "outputId": "72837b22-bc52-4a52-9eda-f0b34dc8e1ad"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Dataset Row Count\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "418"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.show(n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIz1uAq-XJT1",
        "outputId": "e9bed5cd-4bfc-4121-a9f7-c51b81bf7313"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
            "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
            "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
            "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
            "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.show(n=2, truncate=False, vertical=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsiKNRnpXSFM",
        "outputId": "8e298c31-d7bb-45d5-9cca-ee3fe36f4a08"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0----------------------------------------------------------\n",
            " PassengerId | 1                                                   \n",
            " Survived    | 0                                                   \n",
            " Pclass      | 3                                                   \n",
            " Name        | Braund, Mr. Owen Harris                             \n",
            " Sex         | male                                                \n",
            " Age         | 22.0                                                \n",
            " SibSp       | 1                                                   \n",
            " Parch       | 0                                                   \n",
            " Ticket      | A/5 21171                                           \n",
            " Fare        | 7.25                                                \n",
            " Cabin       | null                                                \n",
            " Embarked    | S                                                   \n",
            "-RECORD 1----------------------------------------------------------\n",
            " PassengerId | 2                                                   \n",
            " Survived    | 1                                                   \n",
            " Pclass      | 1                                                   \n",
            " Name        | Cumings, Mrs. John Bradley (Florence Briggs Thayer) \n",
            " Sex         | female                                              \n",
            " Age         | 38.0                                                \n",
            " SibSp       | 1                                                   \n",
            " Parch       | 0                                                   \n",
            " Ticket      | PC 17599                                            \n",
            " Fare        | 71.2833                                             \n",
            " Cabin       | C85                                                 \n",
            " Embarked    | C                                                   \n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.describe().show(3,vertical=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dZm5PEXeAiN",
        "outputId": "10ac0e6c-9c30-470b-8083-20c3ac9e4fb2"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0--------------------------\n",
            " summary     | count               \n",
            " PassengerId | 891                 \n",
            " Survived    | 891                 \n",
            " Pclass      | 891                 \n",
            " Name        | 891                 \n",
            " Sex         | 891                 \n",
            " Age         | 714                 \n",
            " SibSp       | 891                 \n",
            " Parch       | 891                 \n",
            " Ticket      | 891                 \n",
            " Fare        | 891                 \n",
            " Cabin       | 204                 \n",
            " Embarked    | 889                 \n",
            "-RECORD 1--------------------------\n",
            " summary     | mean                \n",
            " PassengerId | 446.0               \n",
            " Survived    | 0.3838383838383838  \n",
            " Pclass      | 2.308641975308642   \n",
            " Name        | null                \n",
            " Sex         | null                \n",
            " Age         | 29.69911764705882   \n",
            " SibSp       | 0.5230078563411896  \n",
            " Parch       | 0.38159371492704824 \n",
            " Ticket      | 260318.54916792738  \n",
            " Fare        | 32.2042079685746    \n",
            " Cabin       | null                \n",
            " Embarked    | null                \n",
            "-RECORD 2--------------------------\n",
            " summary     | stddev              \n",
            " PassengerId | 257.3538420152301   \n",
            " Survived    | 0.48659245426485753 \n",
            " Pclass      | 0.8360712409770491  \n",
            " Name        | null                \n",
            " Sex         | null                \n",
            " Age         | 14.526497332334035  \n",
            " SibSp       | 1.1027434322934315  \n",
            " Parch       | 0.8060572211299488  \n",
            " Ticket      | 471609.26868834975  \n",
            " Fare        | 49.69342859718089   \n",
            " Cabin       | null                \n",
            " Embarked    | null                \n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check for any nulls values."
      ],
      "metadata": {
        "id": "oL3fDbPDR-p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of null values\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "print (\"NaN values\\n\")\n",
        "training_dataset.select([count(when(isnan(item), item)).alias(item) for item in training_dataset.columns]).show(5)\n",
        "\n",
        "print (\"Null values\\n\")\n",
        "training_dataset.select([count(when(col(item).isNull(), item)).alias(item) for item in training_dataset.columns]).show(5)\n",
        "\n",
        "print (\"Not Null values\\n\")\n",
        "training_dataset.select([count(when(col(item).isNotNull(), item)).alias(item) for item in training_dataset.columns]).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhXdgvnzeJnD",
        "outputId": "59cbe0f0-4c2a-44ac-ddb0-4a331cea585c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values\n",
            "\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|    0|       0|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "\n",
            "Null values\n",
            "\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "\n",
            "Not Null values\n",
            "\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "|        891|     891|   891| 891|891|714|  891|  891|   891| 891|  204|     889|\n",
            "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Renaming Column Name\")\n",
        "training_dataset = training_dataset.withColumnRenamed(\"Pclass\",\"PassengerClasses\").withColumnRenamed(\"Sex\",\"Gender\")\n",
        "training_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb9q3wxTebss",
        "outputId": "419cb43c-f28c-41cb-a883-08b962fd43b2"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renaming Column Name\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[PassengerId: int, Survived: int, PassengerClasses: int, Name: string, Gender: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Counting the number of Passenger per Classes\")\n",
        "training_dataset.groupBy(\"PassengerClasses\").count().sort(\"PassengerClasses\").show()\n",
        "\n",
        "\n",
        "print(\"Counting the number of Survivals by Classes\")\n",
        "training_dataset.groupBy(\"PassengerClasses\",\n",
        "                         \"Gender\",\n",
        "                         \"Survived\").count().sort(\"PassengerClasses\",\n",
        "                                                  \"Gender\",\n",
        "                                                  \"Survived\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4zyvFRl1eoG",
        "outputId": "bb9df4c4-cb43-44d1-ab1e-c270447c1953"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting the number of Passenger per Classes\n",
            "+----------------+-----+\n",
            "|PassengerClasses|count|\n",
            "+----------------+-----+\n",
            "|               1|  216|\n",
            "|               2|  184|\n",
            "|               3|  491|\n",
            "+----------------+-----+\n",
            "\n",
            "Counting the number of Survivals by Classes\n",
            "+----------------+------+--------+-----+\n",
            "|PassengerClasses|Gender|Survived|count|\n",
            "+----------------+------+--------+-----+\n",
            "|               1|female|       0|    3|\n",
            "|               1|female|       1|   91|\n",
            "|               1|  male|       0|   77|\n",
            "|               1|  male|       1|   45|\n",
            "|               2|female|       0|    6|\n",
            "|               2|female|       1|   70|\n",
            "|               2|  male|       0|   91|\n",
            "|               2|  male|       1|   17|\n",
            "|               3|female|       0|   72|\n",
            "|               3|female|       1|   72|\n",
            "|               3|  male|       0|  300|\n",
            "|               3|  male|       1|   47|\n",
            "+----------------+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "OXhwrM_WZQc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Name' column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the 'withColumn' operation."
      ],
      "metadata": {
        "id": "kNsXPCLyZ3Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = training_dataset.withColumn(\"Title\", regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\", 1))\n",
        "training_dataset.select(\"Name\",\"Title\").show(10) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOGu71u7Wrq3",
        "outputId": "8e68c466-6c92-4896-dcd2-81ffefcee81a"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|                Name| Title|\n",
            "+--------------------+------+\n",
            "|Braund, Mr. Owen ...|    Mr|\n",
            "|Cumings, Mrs. Joh...|   Mrs|\n",
            "|Heikkinen, Miss. ...|  Miss|\n",
            "|Futrelle, Mrs. Ja...|   Mrs|\n",
            "|Allen, Mr. Willia...|    Mr|\n",
            "|    Moran, Mr. James|    Mr|\n",
            "|McCarthy, Mr. Tim...|    Mr|\n",
            "|Palsson, Master. ...|Master|\n",
            "|Johnson, Mrs. Osc...|   Mrs|\n",
            "|Nasser, Mrs. Nich...|   Mrs|\n",
            "+--------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset.groupBy(\"Title\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5C0dIcs1niB",
        "outputId": "c605d69f-9cf2-43cf-c4bf-d456578c8e3a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|   Title|count|\n",
            "+--------+-----+\n",
            "|     Don|    1|\n",
            "|    Miss|  182|\n",
            "|Countess|    1|\n",
            "|     Col|    2|\n",
            "|     Rev|    6|\n",
            "|    Lady|    1|\n",
            "|  Master|   40|\n",
            "|     Mme|    1|\n",
            "|    Capt|    1|\n",
            "|      Mr|  517|\n",
            "|      Dr|    7|\n",
            "|     Mrs|  125|\n",
            "|     Sir|    1|\n",
            "|Jonkheer|    1|\n",
            "|    Mlle|    2|\n",
            "|   Major|    2|\n",
            "|      Ms|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Name` column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the `withColumn` operation."
      ],
      "metadata": {
        "id": "wjTI1nYE1rO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = training_dataset.\\\n",
        "replace([\"Mme\", \n",
        "         \"Mlle\",\"Ms\",\n",
        "         \"Major\",\"Dr\", \"Capt\",\"Col\",\"Rev\",\n",
        "         \"Lady\",\"Dona\", \"the Countess\",\"Countess\", \"Don\", \"Sir\", \"Jonkheer\",\"Master\"],\n",
        "        [\"Mrs\", \n",
        "         \"Miss\", \"Miss\",\n",
        "         \"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\n",
        "         \"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\"])\n",
        "\n",
        "feature_df.groupBy(\"Title\").count().sort(desc(\"count\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUJnIias1qqq",
        "outputId": "8758bf6b-df71-4d08-f97a-f91fed992ccd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  Title|count|\n",
            "+-------+-----+\n",
            "|     Mr|  517|\n",
            "|   Miss|  185|\n",
            "|    Mrs|  126|\n",
            "|Royalty|   45|\n",
            "| Ranked|   18|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some duplicated or misspelled writer names may exist. You may replace them by using the function 'replace' as the following."
      ],
      "metadata": {
        "id": "FnVY0raraGMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dataframe.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2yr2BWRiTzU",
        "outputId": "afcd4086-02da-4801-d47d-9ded6cd1c670"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_id', 'struct<$oid:string>'),\n",
              " ('amazon_product_url', 'string'),\n",
              " ('author', 'string'),\n",
              " ('bestsellers_date', 'struct<$date:struct<$numberLong:string>>'),\n",
              " ('description', 'string'),\n",
              " ('price', 'struct<$numberDouble:string,$numberInt:string>'),\n",
              " ('published_date', 'struct<$date:struct<$numberLong:string>>'),\n",
              " ('publisher', 'string'),\n",
              " ('rank', 'struct<$numberInt:string>'),\n",
              " ('rank_last_week', 'struct<$numberInt:string>'),\n",
              " ('title', 'string'),\n",
              " ('weeks_on_list', 'struct<$numberInt:string>'),\n",
              " ('writer', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = feature_df.select(\n",
        "    \"Survived\",\n",
        "    \"PassengerClasses\",\n",
        "    \"SibSp\",\n",
        "    \"Parch\")\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.fillna(0)\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifNzLhgZ2rw",
        "outputId": "085ab18a-689a-4cdb-ee1d-4203b237958f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Survived', 'int'),\n",
              " ('PassengerClasses', 'int'),\n",
              " ('SibSp', 'int'),\n",
              " ('Parch', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## String Indexing"
      ],
      "metadata": {
        "id": "aOmeR2uojUBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting the model implementation stage, the formats of all features should be inspected. Since the prediction method requires numerical variables, string-formatted columns shall be all converted into corresponding numerical types in the final modeling dataset."
      ],
      "metadata": {
        "id": "0QbOYj97afyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "parchIndexer = StringIndexer(inputCol=\"Parch\", outputCol=\"Parch_Ind\").fit(df)\n",
        "sibspIndexer = StringIndexer(inputCol=\"SibSp\", outputCol=\"SibSp_Ind\").fit(df)\n",
        "passangerIndexer = StringIndexer(inputCol=\"PassengerClasses\", outputCol=\"PassengerClasses_Ind\").fit(df)\n",
        "survivedIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_Ind\").fit(df)"
      ],
      "metadata": {
        "id": "057n-AMMawle"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Assembler"
      ],
      "metadata": {
        "id": "epUuRG2nlkyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the indexing and dropping of old string-formatted operations, the DataFrame has all numerical variables. Since all the columns have a non-string format, we can generate a feature vector using the columns in the DataFrame. The 'VectorAssembler' can be applied to transform the 'features' vector column."
      ],
      "metadata": {
        "id": "r5ZJx5MzlZCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "  inputCols = [\"PassengerClasses\",\"SibSp\",\"Parch\"],\n",
        "  outputCol = \"features\")"
      ],
      "metadata": {
        "id": "XFD1fBwSlrKB"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Train/Test"
      ],
      "metadata": {
        "id": "pEMQriJ7nIdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'randomSplit' method can be used to divide the data into train and test sets. "
      ],
      "metadata": {
        "id": "YxpHiZ9dmArG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train, test) = df.randomSplit([0.8, 0.2], seed = 345)"
      ],
      "metadata": {
        "id": "foLSJ-eVl63q"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling with Spark MLlib"
      ],
      "metadata": {
        "id": "m5SChtsPaZ64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Classifier"
      ],
      "metadata": {
        "id": "sl6MM41CpgNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
        "classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7UAtmPFpjeP",
        "outputId": "f0a5b9b0-5aa0-4d06-f32a-72d0379cec38"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier_9e7c1bc1eef5"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Pipeline"
      ],
      "metadata": {
        "id": "vnvthnBiqXIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, classifier, parchIndexer, sibspIndexer, passangerIndexer, survivedIndexer])\n",
        "pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abZoOAyZqVSY",
        "outputId": "fbb5f0b3-50dc-4d12-cbb8-7105cc0cdb7e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline_9208a01fc936"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Prepare training with ParamGridBuilder"
      ],
      "metadata": {
        "id": "gYkrXk6-nvBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the pipeline is created, the parameters of the classifier can be optimized with the help of 'ParamGridBuilder'. Corresponding parameters will be created after the grid search."
      ],
      "metadata": {
        "id": "BDz_q8d7mul4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import TrainValidationSplit\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "  .addGrid(classifier.maxDepth, [5, 10, 15, 20]) \\\n",
        "  .addGrid(classifier.maxBins, [25, 30]) \\\n",
        "  .build()\n",
        "paramGrid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuQBZYwYmu2h",
        "outputId": "dda55b67-28d9-4c31-926c-a2ae9796ad80"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20},\n",
              " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20}]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this respect, the 'label', 'features', and 'metric' columns can be applied."
      ],
      "metadata": {
        "id": "CFcXQuSap9Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tvs = TrainValidationSplit(\n",
        "  estimator=pipeline,\n",
        "  estimatorParamMaps=paramGrid,\n",
        "  evaluator=MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"),\n",
        "  trainRatio=0.8)\n",
        "\n",
        "tvs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoZfigr4qDTE",
        "outputId": "7c0707bb-ac66-4609-d330-a7cd5fd26fca"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainValidationSplit_93dc7ce09291"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model Fitting"
      ],
      "metadata": {
        "id": "7WtqOSsSsltc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the 'TrainValidationSplit' phase is finalized, the model can be fitted."
      ],
      "metadata": {
        "id": "MiJexsgIrF3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_generated = tvs.fit(train)"
      ],
      "metadata": {
        "id": "B3dhsIGOrJj_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "FbxYJ9lUs1j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print accuracy results by each parameter"
      ],
      "metadata": {
        "id": "PvpD0PUCs5Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(model_generated.validationMetrics, model_generated.getEstimatorParamMaps()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmNRoDUhs3bB",
        "outputId": "5bcee640-629b-4ed2-c416-f95498010470"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.7113061435209086,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}),\n",
              " (0.7113061435209086,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20}),\n",
              " (0.6858601215725474,\n",
              "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
              "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20})]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Serving with MLFlow"
      ],
      "metadata": {
        "id": "nCy09-Kor9P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models generated using PySpark can be distributed with the help of the software package MLFlow. "
      ],
      "metadata": {
        "id": "FGva74oStArk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_kbYHvitGY8",
        "outputId": "99c0f716-977a-4a1c-bdd6-eaef3b06d532"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.8 MB 476 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting prometheus-flask-exporter\n",
            "  Downloading prometheus_flask_exporter-0.20.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (4.11.3)\n",
            "Collecting docker>=4.0.0\n",
            "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.36)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.1)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting gitpython>=2.1.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
            "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 776 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2022.1)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.2)\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow) (5.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.14.1)\n",
            "Building wheels for collected packages: databricks-cli\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=6e40e6b88b2764dda4c869e1f6ec349ea9249e1c3dfd1956eb407adeffd5f1c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
            "Successfully built databricks-cli\n",
            "Installing collected packages: smmap, websocket-client, pyjwt, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Mako-1.2.0 alembic-1.7.7 databricks-cli-0.16.6 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 mlflow-1.26.1 prometheus-flask-exporter-0.20.1 pyjwt-2.4.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution of MLFlow"
      ],
      "metadata": {
        "id": "kI4jVSiatNsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may run the 'start_run()' function after importing MLflow to activate MLflow in a Spark session."
      ],
      "metadata": {
        "id": "QerVkTpCtQ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from mlflow import spark\n",
        "with mlflow.start_run(): \n",
        "    model = tvs.fit(train) \n",
        "    mlflow.spark.log_model(model_generated, \"sparkML-model\")"
      ],
      "metadata": {
        "id": "FKjVTuYIwRnF"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding model inferences can be occupied by using the `mlflow.pyfunc` function. For this purpose, it is crucial to assign the model and dataset paths separately. Then, a Spark UDF can be generated by using the model path. The next step is to read and register them into a dataframe. For the final phase, a new feature is created with the help of the formerly defined Spark UDF."
      ],
      "metadata": {
        "id": "3AGsP_IDyVlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow.pyfunc\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "train.toPandas().to_csv('dataset.csv')\n",
        "\n",
        "model_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/mlruns/1/51ef199ab3b945e8a31b47cdfbf60912/artifacts/sparkML-model'\n",
        "titanic_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/dataset.csv'\n",
        "titanic_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").option('delimiter', ';').load(titanic_path)\n",
        "\n",
        "columns = ['PassengerClasses', 'SibSp', 'Parch']\n",
        "          \n",
        "df.withColumn('Inferences', titanic_udf(*columns)).show(False)"
      ],
      "metadata": {
        "id": "jI_wpjL4wTxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "id": "umu7h8Vy26RD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}