{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSwtcPsoPdg9"
   },
   "source": [
    "# PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeV4tzG_Qrdq"
   },
   "source": [
    "Apache Spark is a system that provides a cluster-based distributed computing environment with the help of its packages, including:\n",
    "*   SQL querying\n",
    "*   streaming data processing\n",
    "*   machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c9YdS_BQ7OY"
   },
   "source": [
    "## Spark Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-ZkTwaRQLvo",
    "outputId": "a693e5dc-62cc-437e-c277-e49cbeff8ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 54.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=eb8aaf73956cd515281bdbd2575d803aa38c0a73265759a2f31f428788344898\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4P9oNIPRB6-"
   },
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4OJ4WaHQQXX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a35H1z7URElW"
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"FirstSparkApplication\").config (\"spark.executor.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpyBMPu6RQbi"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7fTDW9iRlpt"
   },
   "source": [
    "The “Titanic” dataset will be used and may be downloaded from Kaggle website [here](https://www.kaggle.com/c/titanic/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsnVcgiaQKLo",
    "outputId": "0de28809-5f48-4845-94ed-c95ed780345f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('/content/titanic_train.csv')\n",
    "test_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('/content/test.csv')\n",
    "\n",
    "training_dataset.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb9q3wxTebss",
    "outputId": "419cb43c-f28c-41cb-a883-08b962fd43b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Column Name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, PassengerClasses: int, Name: string, Gender: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Renaming Column Name\")\n",
    "training_dataset = training_dataset.withColumnRenamed(\"Pclass\",\"PassengerClasses\").withColumnRenamed(\"Sex\",\"Gender\")\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhwrM_WZQc1"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNsXPCLyZ3Tr"
   },
   "source": [
    "The 'Name' column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the 'withColumn' operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOGu71u7Wrq3",
    "outputId": "8e68c466-6c92-4896-dcd2-81ffefcee81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Name| Title|\n",
      "+--------------------+------+\n",
      "|Braund, Mr. Owen ...|    Mr|\n",
      "|Cumings, Mrs. Joh...|   Mrs|\n",
      "|Heikkinen, Miss. ...|  Miss|\n",
      "|Futrelle, Mrs. Ja...|   Mrs|\n",
      "|Allen, Mr. Willia...|    Mr|\n",
      "|    Moran, Mr. James|    Mr|\n",
      "|McCarthy, Mr. Tim...|    Mr|\n",
      "|Palsson, Master. ...|Master|\n",
      "|Johnson, Mrs. Osc...|   Mrs|\n",
      "|Nasser, Mrs. Nich...|   Mrs|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset = training_dataset.withColumn(\"Title\", regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\", 1))\n",
    "training_dataset.select(\"Name\",\"Title\").show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5C0dIcs1niB",
    "outputId": "c605d69f-9cf2-43cf-c4bf-d456578c8e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   Title|count|\n",
      "+--------+-----+\n",
      "|     Don|    1|\n",
      "|    Miss|  182|\n",
      "|Countess|    1|\n",
      "|     Col|    2|\n",
      "|     Rev|    6|\n",
      "|    Lady|    1|\n",
      "|  Master|   40|\n",
      "|     Mme|    1|\n",
      "|    Capt|    1|\n",
      "|      Mr|  517|\n",
      "|      Dr|    7|\n",
      "|     Mrs|  125|\n",
      "|     Sir|    1|\n",
      "|Jonkheer|    1|\n",
      "|    Mlle|    2|\n",
      "|   Major|    2|\n",
      "|      Ms|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.groupBy(\"Title\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjTI1nYE1rO6"
   },
   "source": [
    "The `Name` column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the `withColumn` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUJnIias1qqq",
    "outputId": "8758bf6b-df71-4d08-f97a-f91fed992ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  Title|count|\n",
      "+-------+-----+\n",
      "|     Mr|  517|\n",
      "|   Miss|  185|\n",
      "|    Mrs|  126|\n",
      "|Royalty|   45|\n",
      "| Ranked|   18|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = training_dataset.\\\n",
    "replace([\"Mme\", \n",
    "         \"Mlle\",\"Ms\",\n",
    "         \"Major\",\"Dr\", \"Capt\",\"Col\",\"Rev\",\n",
    "         \"Lady\",\"Dona\", \"the Countess\",\"Countess\", \"Don\", \"Sir\", \"Jonkheer\",\"Master\"],\n",
    "        [\"Mrs\", \n",
    "         \"Miss\", \"Miss\",\n",
    "         \"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\n",
    "         \"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\"])\n",
    "\n",
    "feature_df.groupBy(\"Title\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnVY0raraGMX"
   },
   "source": [
    "Some duplicated or misspelled writer names may exist. You may replace them by using the function 'replace' as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2yr2BWRiTzU",
    "outputId": "afcd4086-02da-4801-d47d-9ded6cd1c670"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'struct<$oid:string>'),\n",
       " ('amazon_product_url', 'string'),\n",
       " ('author', 'string'),\n",
       " ('bestsellers_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('description', 'string'),\n",
       " ('price', 'struct<$numberDouble:string,$numberInt:string>'),\n",
       " ('published_date', 'struct<$date:struct<$numberLong:string>>'),\n",
       " ('publisher', 'string'),\n",
       " ('rank', 'struct<$numberInt:string>'),\n",
       " ('rank_last_week', 'struct<$numberInt:string>'),\n",
       " ('title', 'string'),\n",
       " ('weeks_on_list', 'struct<$numberInt:string>'),\n",
       " ('writer', 'string')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ifNzLhgZ2rw",
    "outputId": "085ab18a-689a-4cdb-ee1d-4203b237958f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Survived', 'int'),\n",
       " ('PassengerClasses', 'int'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = feature_df.select(\n",
    "    \"Survived\",\n",
    "    \"PassengerClasses\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\")\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.fillna(0)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmeR2uojUBp"
   },
   "source": [
    "## String Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QbOYj97afyR"
   },
   "source": [
    "Before starting the model implementation stage, the formats of all features should be inspected. Since the prediction method requires numerical variables, string-formatted columns shall be all converted into corresponding numerical types in the final modeling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "057n-AMMawle"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "parchIndexer = StringIndexer(inputCol=\"Parch\", outputCol=\"Parch_Ind\").fit(df)\n",
    "sibspIndexer = StringIndexer(inputCol=\"SibSp\", outputCol=\"SibSp_Ind\").fit(df)\n",
    "passangerIndexer = StringIndexer(inputCol=\"PassengerClasses\", outputCol=\"PassengerClasses_Ind\").fit(df)\n",
    "survivedIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_Ind\").fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epUuRG2nlkyJ"
   },
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5ZJx5MzlZCL"
   },
   "source": [
    "After the indexing and dropping of old string-formatted operations, the DataFrame has all numerical variables. Since all the columns have a non-string format, we can generate a feature vector using the columns in the DataFrame. The 'VectorAssembler' can be applied to transform the 'features' vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFD1fBwSlrKB"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [\"PassengerClasses\",\"SibSp\",\"Parch\"],\n",
    "  outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEMQriJ7nIdW"
   },
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxpHiZ9dmArG"
   },
   "source": [
    "The 'randomSplit' method can be used to divide the data into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foLSJ-eVl63q"
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2], seed = 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5SChtsPaZ64"
   },
   "source": [
    "# Modeling with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl6MM41CpgNo"
   },
   "source": [
    "## Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7UAtmPFpjeP",
    "outputId": "f0a5b9b0-5aa0-4d06-f32a-72d0379cec38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_9e7c1bc1eef5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnvthnBiqXIb"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abZoOAyZqVSY",
    "outputId": "fbb5f0b3-50dc-4d12-cbb8-7105cc0cdb7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_9208a01fc936"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, classifier, parchIndexer, sibspIndexer, passangerIndexer, survivedIndexer])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYkrXk6-nvBL"
   },
   "source": [
    "\n",
    "## Prepare training with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDz_q8d7mul4"
   },
   "source": [
    "When the pipeline is created, the parameters of the classifier can be optimized with the help of 'ParamGridBuilder'. Corresponding parameters will be created after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuQBZYwYmu2h",
    "outputId": "dda55b67-28d9-4c31-926c-a2ae9796ad80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20},\n",
       " {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "  Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(classifier.maxDepth, [5, 10, 15, 20]) \\\n",
    "  .addGrid(classifier.maxBins, [25, 30]) \\\n",
    "  .build()\n",
    "paramGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFcXQuSap9Ky"
   },
   "source": [
    "With this respect, the 'label', 'features', and 'metric' columns can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoZfigr4qDTE",
    "outputId": "7c0707bb-ac66-4609-d330-a7cd5fd26fca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainValidationSplit_93dc7ce09291"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "  estimator=pipeline,\n",
    "  estimatorParamMaps=paramGrid,\n",
    "  evaluator=MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"),\n",
    "  trainRatio=0.8)\n",
    "\n",
    "tvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WtqOSsSsltc"
   },
   "source": [
    "\n",
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiJexsgIrF3R"
   },
   "source": [
    "When the 'TrainValidationSplit' phase is finalized, the model can be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3dhsIGOrJj_"
   },
   "outputs": [],
   "source": [
    "model_generated = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbxYJ9lUs1j3"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvpD0PUCs5Bd"
   },
   "source": [
    "Print accuracy results by each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmNRoDUhs3bB",
    "outputId": "5bcee640-629b-4ed2-c416-f95498010470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7113061435209086,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}),\n",
       " (0.7113061435209086,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20}),\n",
       " (0.6858601215725474,\n",
       "  {Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30,\n",
       "   Param(parent='DecisionTreeClassifier_9e7c1bc1eef5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20})]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model_generated.validationMetrics, model_generated.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCy09-Kor9P0"
   },
   "source": [
    "# Model Serving with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGva74oStArk"
   },
   "source": [
    "Machine learning models generated using PySpark can be distributed with the help of the software package MLFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_kbYHvitGY8",
    "outputId": "99c0f716-977a-4a1c-bdd6-eaef3b06d532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.8 MB 476 kB/s \n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (7.1.2)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 49.2 MB/s \n",
      "\u001b[?25hCollecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.20.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 58.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (4.11.3)\n",
      "Collecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 49.1 MB/s \n",
      "\u001b[?25hCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.7/dist-packages (from mlflow) (2.23.0)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.36)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.21.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow) (21.3)\n",
      "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.1.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlflow) (1.4.1)\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n",
      "\u001b[?25hCollecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 52.2 MB/s \n",
      "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 776 kB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow) (3.17.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from mlflow) (2022.1)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow) (0.4.2)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow) (4.2.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2022.5.18.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 7.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow) (5.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow) (1.1.2)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.0.1)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow) (57.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mlflow) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->mlflow) (2.8.2)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow) (0.14.1)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=6e40e6b88b2764dda4c869e1f6ec349ea9249e1c3dfd1956eb407adeffd5f1c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: smmap, websocket-client, pyjwt, Mako, gitdb, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, alembic, mlflow\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed Mako-1.2.0 alembic-1.7.7 databricks-cli-0.16.6 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 mlflow-1.26.1 prometheus-flask-exporter-0.20.1 pyjwt-2.4.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 websocket-client-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI4jVSiatNsc"
   },
   "source": [
    "## Execution of MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QerVkTpCtQ1U"
   },
   "source": [
    "You may run the 'start_run()' function after importing MLflow to activate MLflow in a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKjVTuYIwRnF"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import spark\n",
    "with mlflow.start_run(): \n",
    "    model = tvs.fit(train) \n",
    "    mlflow.spark.log_model(model_generated, \"sparkML-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AGsP_IDyVlX"
   },
   "source": [
    "The corresponding model inferences can be occupied by using the `mlflow.pyfunc` function. For this purpose, it is crucial to assign the model and dataset paths separately. Then, a Spark UDF can be generated by using the model path. The next step is to read and register them into a dataframe. For the final phase, a new feature is created with the help of the formerly defined Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI_wpjL4wTxt"
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "train.toPandas().to_csv('dataset.csv')\n",
    "\n",
    "model_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/mlruns/1/51ef199ab3b945e8a31b47cdfbf60912/artifacts/sparkML-model'\n",
    "titanic_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/dataset.csv'\n",
    "titanic_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").option('delimiter', ';').load(titanic_path)\n",
    "\n",
    "columns = ['PassengerClasses', 'SibSp', 'Parch']\n",
    "          \n",
    "df.withColumn('Inferences', titanic_udf(*columns)).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umu7h8Vy26RD"
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN9q+Etk1pvx+9X4EMiKPrK",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "titanic_spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
