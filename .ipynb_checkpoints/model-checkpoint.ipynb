{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSwtcPsoPdg9"
   },
   "source": [
    "# PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeV4tzG_Qrdq"
   },
   "source": [
    "Apache Spark is a system that provides a cluster-based distributed computing environment with the help of its packages, including:\n",
    "*   SQL querying\n",
    "*   streaming data processing\n",
    "*   machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4P9oNIPRB6-"
   },
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I4OJ4WaHQQXX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a35H1z7URElW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-28 17:52:09 WARN  Utils:66 - Your hostname, siul resolves to a loopback address: 127.0.1.1; using 172.25.140.151 instead (on interface eth0)\n",
      "2022-05-28 17:52:09 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2022-05-28 17:52:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"FirstSparkApplication\").config (\"spark.executor.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpyBMPu6RQbi"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7fTDW9iRlpt"
   },
   "source": [
    "The “Titanic” dataset will be used and may be downloaded from Kaggle website [here](https://www.kaggle.com/c/titanic/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsnVcgiaQKLo",
    "outputId": "0de28809-5f48-4845-94ed-c95ed780345f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('data/train.csv')\n",
    "test_dataset = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").load('data/test.csv')\n",
    "\n",
    "training_dataset.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb9q3wxTebss",
    "outputId": "419cb43c-f28c-41cb-a883-08b962fd43b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Column Name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, PassengerClasses: int, Name: string, Gender: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Renaming Column Name\")\n",
    "training_dataset = training_dataset.withColumnRenamed(\"Pclass\",\"PassengerClasses\").withColumnRenamed(\"Sex\",\"Gender\")\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhwrM_WZQc1"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNsXPCLyZ3Tr"
   },
   "source": [
    "The 'Name' column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the 'withColumn' operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOGu71u7Wrq3",
    "outputId": "8e68c466-6c92-4896-dcd2-81ffefcee81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Name| Title|\n",
      "+--------------------+------+\n",
      "|Braund, Mr. Owen ...|    Mr|\n",
      "|Cumings, Mrs. Joh...|   Mrs|\n",
      "|Heikkinen, Miss. ...|  Miss|\n",
      "|Futrelle, Mrs. Ja...|   Mrs|\n",
      "|Allen, Mr. Willia...|    Mr|\n",
      "|    Moran, Mr. James|    Mr|\n",
      "|McCarthy, Mr. Tim...|    Mr|\n",
      "|Palsson, Master. ...|Master|\n",
      "|Johnson, Mrs. Osc...|   Mrs|\n",
      "|Nasser, Mrs. Nich...|   Mrs|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset = training_dataset.withColumn(\"Title\", regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\", 1))\n",
    "training_dataset.select(\"Name\",\"Title\").show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjTI1nYE1rO6"
   },
   "source": [
    "The `Name` column in the titanic dataset also includes the person’s title. This information might be beneficial in the model. So let’s generate it as a new variable. A new title column can be created using the `withColumn` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUJnIias1qqq",
    "outputId": "8758bf6b-df71-4d08-f97a-f91fed992ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  Title|count|\n",
      "+-------+-----+\n",
      "|     Mr|  517|\n",
      "|   Miss|  185|\n",
      "|    Mrs|  126|\n",
      "|Royalty|   45|\n",
      "| Ranked|   18|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = training_dataset.\\\n",
    "replace([\"Mme\", \n",
    "         \"Mlle\",\"Ms\",\n",
    "         \"Major\",\"Dr\", \"Capt\",\"Col\",\"Rev\",\n",
    "         \"Lady\",\"Dona\", \"the Countess\",\"Countess\", \"Don\", \"Sir\", \"Jonkheer\",\"Master\"],\n",
    "        [\"Mrs\", \n",
    "         \"Miss\", \"Miss\",\n",
    "         \"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\"Ranked\",\n",
    "         \"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\",\"Royalty\"])\n",
    "\n",
    "feature_df.groupBy(\"Title\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnVY0raraGMX"
   },
   "source": [
    "Some duplicated or misspelled writer names may exist. You may replace them by using the function 'replace' as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2yr2BWRiTzU",
    "outputId": "afcd4086-02da-4801-d47d-9ded6cd1c670"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('PassengerClasses', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'double'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string'),\n",
       " ('Title', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ifNzLhgZ2rw",
    "outputId": "085ab18a-689a-4cdb-ee1d-4203b237958f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Survived', 'int'),\n",
       " ('PassengerClasses', 'int'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = feature_df.select(\n",
    "    \"Survived\",\n",
    "    \"PassengerClasses\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\")\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.fillna(0)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOmeR2uojUBp"
   },
   "source": [
    "## String Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QbOYj97afyR"
   },
   "source": [
    "Before starting the model implementation stage, the formats of all features should be inspected. Since the prediction method requires numerical variables, string-formatted columns shall be all converted into corresponding numerical types in the final modeling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "057n-AMMawle"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "parchIndexer = StringIndexer(inputCol=\"Parch\", outputCol=\"Parch_Ind\").fit(df)\n",
    "sibspIndexer = StringIndexer(inputCol=\"SibSp\", outputCol=\"SibSp_Ind\").fit(df)\n",
    "passangerIndexer = StringIndexer(inputCol=\"PassengerClasses\", outputCol=\"PassengerClasses_Ind\").fit(df)\n",
    "survivedIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"Survived_Ind\").fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epUuRG2nlkyJ"
   },
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5ZJx5MzlZCL"
   },
   "source": [
    "After the indexing and dropping of old string-formatted operations, the DataFrame has all numerical variables. Since all the columns have a non-string format, we can generate a feature vector using the columns in the DataFrame. The 'VectorAssembler' can be applied to transform the 'features' vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XFD1fBwSlrKB"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "  inputCols = [\"PassengerClasses\",\"SibSp\",\"Parch\"],\n",
    "  outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEMQriJ7nIdW"
   },
   "source": [
    "## Split Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxpHiZ9dmArG"
   },
   "source": [
    "The 'randomSplit' method can be used to divide the data into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "foLSJ-eVl63q"
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2], seed = 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5SChtsPaZ64"
   },
   "source": [
    "# Modeling with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl6MM41CpgNo"
   },
   "source": [
    "## Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7UAtmPFpjeP",
    "outputId": "f0a5b9b0-5aa0-4d06-f32a-72d0379cec38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_f25ebaa8ff9e"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Survived\")\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnvthnBiqXIb"
   },
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abZoOAyZqVSY",
    "outputId": "fbb5f0b3-50dc-4d12-cbb8-7105cc0cdb7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_afbcbf0614e0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, classifier, parchIndexer, sibspIndexer, passangerIndexer, survivedIndexer])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYkrXk6-nvBL"
   },
   "source": [
    "\n",
    "## Prepare training with ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDz_q8d7mul4"
   },
   "source": [
    "When the pipeline is created, the parameters of the classifier can be optimized with the help of 'ParamGridBuilder'. Corresponding parameters will be created after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuQBZYwYmu2h",
    "outputId": "dda55b67-28d9-4c31-926c-a2ae9796ad80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25},\n",
       " {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "  Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(classifier.maxDepth, [5, 10, 15, 20]) \\\n",
    "  .addGrid(classifier.maxBins, [25, 30]) \\\n",
    "  .build()\n",
    "paramGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFcXQuSap9Ky"
   },
   "source": [
    "With this respect, the 'label', 'features', and 'metric' columns can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoZfigr4qDTE",
    "outputId": "7c0707bb-ac66-4609-d330-a7cd5fd26fca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainValidationSplit_43054fe595d4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "  estimator=pipeline,\n",
    "  estimatorParamMaps=paramGrid,\n",
    "  evaluator=MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"),\n",
    "  trainRatio=0.8)\n",
    "\n",
    "tvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WtqOSsSsltc"
   },
   "source": [
    "\n",
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiJexsgIrF3R"
   },
   "source": [
    "When the 'TrainValidationSplit' phase is finalized, the model can be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B3dhsIGOrJj_"
   },
   "outputs": [],
   "source": [
    "model_generated = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbxYJ9lUs1j3"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvpD0PUCs5Bd"
   },
   "source": [
    "Print accuracy results by each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmNRoDUhs3bB",
    "outputId": "5bcee640-629b-4ed2-c416-f95498010470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.735344132067275,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25}),\n",
       " (0.735344132067275,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 15,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 25}),\n",
       " (0.7359553281797201,\n",
       "  {Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       "   Param(parent='DecisionTreeClassifier_f25ebaa8ff9e', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 30})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model_generated.validationMetrics, model_generated.getEstimatorParamMaps()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCy09-Kor9P0"
   },
   "source": [
    "# Model Serving with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGva74oStArk"
   },
   "source": [
    "Machine learning models generated using PySpark can be distributed with the help of the software package MLFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI4jVSiatNsc"
   },
   "source": [
    "## Execution of MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QerVkTpCtQ1U"
   },
   "source": [
    "You may run the 'start_run()' function after importing MLflow to activate MLflow in a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "FKjVTuYIwRnF"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24762/1895483542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import mlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"
     ]
    }
   ],
   "source": [
    "# import mlflow\n",
    "\n",
    "from mlflow import spark\n",
    "with mlflow.start_run(): \n",
    "    model = tvs.fit(train) \n",
    "    mlflow.spark.log_model(model_generated, \"sparkML-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AGsP_IDyVlX"
   },
   "source": [
    "The corresponding model inferences can be occupied by using the `mlflow.pyfunc` function. For this purpose, it is crucial to assign the model and dataset paths separately. Then, a Spark UDF can be generated by using the model path. The next step is to read and register them into a dataframe. For the final phase, a new feature is created with the help of the formerly defined Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI_wpjL4wTxt"
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "train.toPandas().to_csv('dataset.csv')\n",
    "\n",
    "model_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/mlruns/1/51ef199ab3b945e8a31b47cdfbf60912/artifacts/sparkML-model'\n",
    "titanic_path = '/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/dataset.csv'\n",
    "titanic_udf = mlflow.pyfunc.spark_udf(spark, model_path)\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", \"true\").option('delimiter', ';').load(titanic_path)\n",
    "\n",
    "columns = ['PassengerClasses', 'SibSp', 'Parch']\n",
    "          \n",
    "df.withColumn('Inferences', titanic_udf(*columns)).show(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umu7h8Vy26RD"
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN9q+Etk1pvx+9X4EMiKPrK",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "titanic_spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
